{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "122pk3XlCega"
   },
   "source": [
    "# Full pipeline Reddit analysis    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e92-XR8VCegm",
    "outputId": "b22666c6-c8b5-4766-8f3f-6451b2a07fb9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import nltk.collocations\n",
    "from nltk.util import ngrams\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kaBz5CmCegn"
   },
   "source": [
    "# 1 Data gathering\n",
    "\n",
    "\n",
    "See pushshift_data_collection.ipynb\n",
    "\n",
    "--> four csv files, two manosphere, two female strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WR5_8Ylw7pO"
   },
   "source": [
    "# 2 Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M7M-vAsCego"
   },
   "source": [
    "### 2.1 Reading csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_1 = pd.read_csv('data_1/fds_all.csv') \n",
    "#data_2 = pd.read_csv('data_1/flus_all.csv')\n",
    "data_1 = pd.read_csv('data/mgtow2_all.csv') \n",
    "data_2 = pd.read_csv('data/mr_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_1, data_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HKI7ryrzGC2"
   },
   "source": [
    "### 2.2 Cleaning and lowercasing   \n",
    "Remove [removed] and [deleted]. In the case of comments, nothing is left. In the case of submissions, the title of the post is still there. \n",
    "Then remove the empty lines from the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'].replace('[removed]', '', inplace=True)\n",
    "data['Text'].replace('[deleted]', '', inplace=True)\n",
    "data['Text'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['Text'], inplace=True)\n",
    "data['Text'] = [x.lower() for x in data['Text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I2QjC0MAF40"
   },
   "source": [
    "### 2.3 Joining all text    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = data['Text'].tolist()\n",
    "text_list = [str(i) for i in text_list]\n",
    "text_string = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgfkqhrCcChn"
   },
   "source": [
    "### 2.4 Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzBGBLOcQsvw"
   },
   "outputs": [],
   "source": [
    "text_string = re.sub(r'[^\\w\\s]','',text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfkHp8eCCegs"
   },
   "source": [
    "### 2.5 Tokenization and stopword removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbS0iH86KgsP",
    "outputId": "a2c06277-0974-4e92-c247-96a06a7c3325"
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop = {'dont', 'hadnt',  'isnt', 'couldnt', 'doesnt', 'youll', 'shouldnt', 'hadnt', 'wont', 'youre', 'mustnt', 'werent', 'wouldnt', 'wasnt', 'hasnt', 'havent', 'shouldve', 'didnt', 'arent', 'shant', 'youve', 'weve', 'shes', 'neednt', 'thall', 'ill', 'ive', 'dont', 'gon', 'na', 'im'}\n",
    "stopset.update(extra_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [w for w in tokens if not w in stopset] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDc06qJkCegt"
   },
   "source": [
    "# 3 Corpus Ling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tKCRhi_Cegt"
   },
   "source": [
    "### 3.1 Word list  \n",
    "Only used for the lexicon analysis and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.DataFrame(list(word_freq.items()),columns = ['Word','Freq_target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF4vaNaVCegu"
   },
   "source": [
    "### 3.2 Keywords   \n",
    "https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/keyword-analysis.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eelb9oN5cDyS"
   },
   "source": [
    "#### 3.2.1 Load reference corpus and preprocess it like the other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_askreddit = data = pd.read_csv('data/askreddit_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "df_askreddit['Text'] = [str(x) for x in df_askreddit['Text']]\n",
    "df_askreddit['Text'] = [x.lower() for x in df_askreddit['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_askreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining\n",
    "text_list_rf = df_askreddit['Text'].tolist()\n",
    "text_list_rf = [str(i) for i in text_list_rf]\n",
    "text_string_rf = ' '.join(text_list_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation\n",
    "text_string_rf = re.sub(r'[^\\w\\s]','',text_string_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokens_rf = nltk.word_tokenize(text_string_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "filtered_tokens_rf = [w for w in tokens_rf if not w in stopset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_tokens_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Word frequencies of reference corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_rf = Counter(filtered_tokens_rf)\n",
    "word_freq_reference = pd.DataFrame(list(word_freq_rf.items()),columns = ['Word','Freq_reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Contingency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.merge(word_freq_df, word_freq_reference, how=\"outer\")\n",
    "contingency_table = contingency_table.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the neccesary columns\n",
    "\n",
    "contingency_table['Rest_target'] = len(tokens) - contingency_table.Freq_target\n",
    "contingency_table['Rest_reference'] = 177409956 - contingency_table.Freq_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing column names to make it easier later on\n",
    "# a is n word in target corpus\n",
    "# b is n word in reference corpus\n",
    "# c all other words in target corpus\n",
    "# d is all other words in reference corpus\n",
    "contingency_table = contingency_table.rename(columns={\"Freq_target\": \"a\", \"Freq_reference\": \"b\", \"Rest_target\": \"c\", \"Rest_reference\": \"d\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating expected frequencies\n",
    "contingency_table['a_exp'] = contingency_table.eval('((a+b)*(a+c))/(a+b+c+d)')\n",
    "contingency_table['b_exp'] = contingency_table.eval('((a+b)*(b+d))/(a+b+c+d)')\n",
    "contingency_table['c_exp'] = contingency_table.eval('((c+d)*(a+c))/(a+b+c+d)')\n",
    "contingency_table['d_exp'] = contingency_table.eval('((c+d)*(b+d))/(a+b+c+d)')\n",
    "\n",
    "# calculating chi-squared\n",
    "contingency_table['Chi2'] = contingency_table.eval('((a-a_exp)**2/a_exp)+((b-b_exp)**2/b_exp)+((c-c_exp)**2/c_exp)+((d-d_exp)**2/d_exp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = contingency_table.sort_values(by='Chi2', ascending=False) # highest chi2 is most key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_keywords = 'output/keywords_female.csv'\n",
    "#filename_keywords = 'output/keywords_mano.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table.to_csv(filename_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(nltk.bigrams(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq_target = pd.DataFrame(list(bigram_freq.items()),columns = ['Bigram','Freq_target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_rf = list(nltk.bigrams(tokens_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigrams_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq_counter = Counter(bigrams_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq_rf = pd.DataFrame(list(bigram_freq_counter.items()),columns = ['Bigram','Freq_reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contingency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_contingency_table = pd.merge(bigram_freq_target, bigram_freq_rf, how=\"outer\")\n",
    "bi_contingency_table = bi_contingency_table.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the neccesary columns\n",
    "\n",
    "bi_contingency_table['Rest_target'] = 25641129 - bi_contingency_table.Freq_target # 25641129 is number of bigrams of target corpus\n",
    "bi_contingency_table['Rest_reference'] = 177409954 - bi_contingency_table.Freq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing column names to make it easier later on\n",
    "# a is n bigram in target corpus\n",
    "# b is n bigram in reference corpus\n",
    "# c all other bigrams in target corpus\n",
    "# d is all other bigrams in reference corpus\n",
    "bi_contingency_table = bi_contingency_table.rename(columns={\"Freq_target\": \"a\", \"Freq_reference\": \"b\", \"Rest_target\": \"c\", \"Rest_reference\": \"d\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating expected frequencies\n",
    "bi_contingency_table['a_exp'] = bi_contingency_table.eval('((a+b)*(a+c))/(a+b+c+d)')\n",
    "bi_contingency_table['b_exp'] = bi_contingency_table.eval('((a+b)*(b+d))/(a+b+c+d)')\n",
    "bi_contingency_table['c_exp'] = bi_contingency_table.eval('((c+d)*(a+c))/(a+b+c+d)')\n",
    "bi_contingency_table['d_exp'] = bi_contingency_table.eval('((c+d)*(b+d))/(a+b+c+d)')\n",
    "\n",
    "# calculating chi-squared\n",
    "bi_contingency_table['Chi2'] = bi_contingency_table.eval('((a-a_exp)**2/a_exp)+((b-b_exp)**2/b_exp)+((c-c_exp)**2/c_exp)+((d-d_exp)**2/d_exp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_contingency_table = bi_contingency_table.sort_values(by='Chi2', ascending=False) # highest chi2 is most key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename_bigrams = 'output/keybigrams_female.csv'\n",
    "filename_bigrams = 'output/keybigrams_mano.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_contingency_table.to_csv(filename_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Gendered words\n",
    "\n",
    "I made smaller files with only sentences that contain some word (men, women, male, female or an abusive term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.remove_pipe('ner') # prevents memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives for men/women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adj = pd.read_csv('data/female_strategy_word_women.csv') \n",
    "#data_adj = pd.read_csv('data/female_strategy_word_men.csv')\n",
    "#data_adj = pd.read_csv('data/manosphere_word_men.csv')\n",
    "#data_adj = pd.read_csv('data/manosphere_word_women.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = data_adj['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_s = set(text_list) #remove duplicates, there can be duplicate sentences when the target word occured more than once in the same sentence\n",
    "text_list = list(text_list_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjectives(target_word, filename_adj):\n",
    "    adj = []\n",
    "    for sentence in text_list:\n",
    "        doc = nlp(sentence)\n",
    "        for possible_adj in doc:\n",
    "            if possible_adj.dep_ == 'amod' and possible_adj.head.text == target_word:\n",
    "                adj.append(possible_adj.text)\n",
    "                adj_count = Counter(adj)\n",
    "                adj_df = pd.DataFrame(list(adj_count.items()),columns = ['ADJ','Freq'])\n",
    "                adj_df.to_csv(filename_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives('women', 'output/female_strategy_adj_women.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives('men', 'output/female_strategy_adj_men.csv') # CHANGE FILE FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives('women', 'output/manosphere_adj_women.csv') # CHANGE FILE FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives('men', 'output/manosphere_adj_men.csv') # CHANGE FILE FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nouns for male/female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noun = pd.read_csv('data/female_strategy_word_male.csv')\n",
    "#data_noun = pd.read_csv('data/female_strategy_word_female.csv')\n",
    "#data_noun = pd.read_csv('data/manosphere_word_male.csv')\n",
    "#data_noun = pd.read_csv('data/manosphere_word_female.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = data['0'].tolist()\n",
    "text_list_s = set(text_list) #remove duplicates\n",
    "text_list = list(text_list_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(target_word, filename_nouns):\n",
    "    noun = []\n",
    "    for sentence in text_list:\n",
    "        doc = nlp(sentence)\n",
    "        for possible_adj in doc:\n",
    "            if possible_adj.text == target_word and possible_adj.head.pos_ == 'NOUN':\n",
    "                noun.append(possible_adj.head.text)\n",
    "                nouns_count = Counter(noun)\n",
    "                nouns_df = pd.DataFrame(list(nouns_count.items()),columns = ['NOUN','Freq'])\n",
    "                nouns_df.to_csv(filename_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns('male', 'output/female_strategy_noun_male.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns('female', 'output/female_strategy_noun_female.csv') # CHANGE FILE FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns('male', 'output/manosphere_noun_male.csv') # CHANGE FILE FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns('female', 'output/manosphere_noun_female.csv') # CHANGE FILE FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2KobcweTmiD"
   },
   "source": [
    "# 4 Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9zjPzHGTnIb"
   },
   "source": [
    "### 4.1 read lexicon and find abuse in text\n",
    "txt file   \n",
    "one word on each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVBG5EgQTun5"
   },
   "outputs": [],
   "source": [
    "def lexicon(filename_lex, dict_abuse, filename_abuse):\n",
    "    with open(filename_lex, 'r', encoding = 'utf-8') as infile:\n",
    "        lexicon = infile.read()\n",
    "        lexicon_list = lexicon.split('\\n')\n",
    "        stripped = [w.strip() for w in lexicon_list] # i noticed that not all words were found, turns out some of the words in the lexicon have a trailing whitespace\n",
    "        for word, freq in word_freq.items():\n",
    "            if word in stripped:\n",
    "                dict_abuse[word] = freq\n",
    "    df_abuse = pd.DataFrame(list(dict_abuse.items()),columns = ['Word','Freq'])\n",
    "    df_abuse.to_csv(filename_abuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misogyny_dict = {}\n",
    "lexicon('../lexicons/abuse_misogyny.txt', misogyny_dict, 'output/manosphere_misogyny.csv')\n",
    "#lexicon('../lexicons/abuse_misogyny.txt', misogyny_dict, 'output/female_strategy_misogyny.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_dict = {}\n",
    "lexicon('../lexicons/abuse_general.txt', general_dict, 'output/manosphere_general.csv')\n",
    "#lexicon('../lexicons/abuse_general.txt', general_dict, 'output/female_strategy_general.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misandry_dict = {}\n",
    "lexicon('../lexicons/abuse_misandry.txt', misandry_dict, 'output/manosphere_misandry.csv')\n",
    "#lexicon('../lexicons/abuse_misandry.txt', misandry_dict, 'output/female_strategy_misandry.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 reduce to short phrases\n",
    "Only shown for female strategy corpus, misogynistic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_miso = pd.read_csv('data/female_strategy_misogyny.csv') #not the same file as above, this is subset of the data with only sentences that contain misogynistic terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = data_miso['0'].tolist()\n",
    "text_list_s = set(text_list)\n",
    "text_list = list(text_list_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### direct object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobj_abuse = []\n",
    "for sentence in text_list:\n",
    "    doc = nlp(sentence)\n",
    "    for word in doc:\n",
    "        if word.text in stripped and word.dep_ == 'dobj':\n",
    "            for child in word.head.children:\n",
    "                if child.dep_ == 'nsubj':\n",
    "                    dobj_abuse.append((child.text, word.head.text, word.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobj_abuse_count = Counter(dobj_abuse)\n",
    "dobj_abuse_df = pd.DataFrame(list(dobj_abuse_count.items()), columns = ['0', '1'])\n",
    "dobj_abuse_df.to_csv('output/female_strategy_miso_dobj.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_abuse = []\n",
    "for sentence in text_list:\n",
    "    doc = nlp(sentence)\n",
    "    for word in doc:\n",
    "        if word.text in stripped and word.dep_ == 'attr':\n",
    "            for child in word.head.children:\n",
    "                if child.dep_ == 'nsubj':\n",
    "                    attr_abuse.append((child.text, word.head.text, word.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_abuse_count = Counter(attr_abuse)\n",
    "attr_abuse_df = pd.DataFrame(list(attr_abuse_count.items()), columns = ['0', '1'])\n",
    "attr_abuse_df.to_csv('output/female_strategy_miso_attr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_abuse = []\n",
    "for sentence in text_list:\n",
    "    doc = nlp(sentence)\n",
    "    for word in doc:\n",
    "        if word.text in stripped and word.dep_ == 'compound': \n",
    "            comp_abuse.append((word.text, word.head.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_abuse_count = Counter(comp_abuse)\n",
    "comp_abuse_df = pd.DataFrame(list(comp_abuse_count.items()), columns = ['0', '1'])\n",
    "comp_abuse_df.to_csv('output/female_strategy_miso_compound.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attributive adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amod_abuse = []\n",
    "for sentence in text_list:\n",
    "    doc = nlp(sentence)\n",
    "    for word in doc:\n",
    "        if word.text in stripped and word.dep_ == 'amod': \n",
    "            amod_abuse.append((word.text, word.head.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amod_abuse_count = Counter(amod_abuse)\n",
    "amod_abuse_df = pd.DataFrame(list(amod_abuse_count.items()), columns = ['0', '1'])\n",
    "amod_abuse_df.to_csv('output/female_strategy_miso_amod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicative adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acomp_abuse = []\n",
    "for sentence in text_list:\n",
    "    doc = nlp(sentence)\n",
    "    for word in doc:\n",
    "        if word.text in stripped and word.dep_ == 'acomp': \n",
    "            for child in word.head.children:\n",
    "                if child.dep_ == 'nsubj':\n",
    "                    acomp_abuse.append((child.text, word.head.text, word.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acomp_abuse_count = Counter(acomp_abuse)\n",
    "acomp_abuse_df = pd.DataFrame(list(acomp_abuse_count.items()), columns = ['0', '1'])\n",
    "acomp_abuse_df.to_csv('output/female_strategy_miso_acomp.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
